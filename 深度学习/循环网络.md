# 循环网络

## 1.1对序列进行建模

### 1.1.1问题提出

在进行建模的时候，常是已知前 $t-1$ 个数据来预测第 $t$ 个数据，用[[条件概率]]表示如下。
$$x_t \sim P(x_t | x_1, x_2,...,x_{t-1})$$
想要预测第 $t$ 个数据就必须知道前 $t-1$ 个数据与第 $t$ 个数之间的关系，所以用函数进行建模，统筹联系。
$$P(x_t | x_1, x_2,...,x_{t-1}) =P(x_t | f(x_1, x_2,...,x_{t-1}))$$
在深度学习中 $f(x_1, x_2,...,x_{t-1})$ 就是我训练的模型。

### 1.1.2解决方案的讨论

#### 1.1.2.1问题阐述

要进行建模需要解决两个问题：

- 如何计算 $f(x_1, x_2,...,x_{t-1}))$
- 如何计算 $x_t$

#### 1.1.2.2提出方案

方案一：**马尔科夫假设**。

利用马尔可夫模型，只取前 $\tau$ 个数据样本来进行预测

方案一：**隐变量自回归**。

利用隐变量 $h_{t}$ 来对 $t$ 时刻之前的数据进行总结，再利用隐变量进行预测。
$$
\hat{x}_t = P(x_t|h_t)
$$
其中隐变量 $h_t$ 与 $x_{t-1}$ 和 $h_{t-1}$ 有关
$$
h_t = g(h_{t-1},x_{t-1})
$$

## 1.2语言模型

### 1.2.1语言模型的目标

语言模型的目标就是计文本序列的联合概率，比如要计算 $P(deep, learning)$ ，用条件概率公式计算就是$P(deep, learning) = P(deep) \cdot P(learning|deep)$，这是预测连续出现的两个单词的概率，在更复杂的语言模型中，要求预测更长的序列，但这变得更加困难。

### 1.2.2马尔可夫模型与n元语法

对于一个预测任务，如果将预测值之前的所有数据都进行计算将会产生巨大的时间复杂度，因此用马尔可夫模型来减少用于计算的序列长度是一个好的选择。

## 1.3困惑度

### 1.3.1定义

困惑度用于衡量语言模型预测正确句子的能力，困惑的反应的是模型预测单个词语的准确程度，而不受句子长度的影响。计算公式如下
$$Perplexity(S)=P(S)^{−\frac{1}{N}} =exp(- \frac{1}{N} \sum_{t=1}^{N}\log{P(x_{t}|x_1,x_2...,x_{t-1}}))$$
其中 $-\frac{1}{N}$ 表示把预测句子的“难度”均摊到每个单词上，以避免由于句子过长导致的困惑度“虚高”。在实际计算中，$P$是由语言模型的$softmax$计算得到的交叉熵。

## 1.4循环神经网络的公式推导

对于神经网络，隐藏层中 $t$ 时刻地隐藏单元与 $t-1$ 时刻的隐藏单元和 $t$ 时刻的输入有关，具体表现为：
$$
H_t = \sigma (X_tW_{xh} + H_{t-1}W{hh} + b_h)
$$
其中 $\sigma$ 表示激活函数，$W_{xh}$ 表示 $X$ 权重矩阵，$W{hh}$ 表示隐藏单元的权重矩阵。

对于输出层，则只与隐藏单元有关
$$
O_t  = H_tW_{hq} + b_{q}
$$
