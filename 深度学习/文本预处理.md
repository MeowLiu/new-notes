## 1.1文本预处理的基本步骤

  一段文本可以看成是一段**单词序列**，对于更简单的文本则可以看成是**字符序列**，常见的文本预处理的步骤如下：

- 将文本作为字符串加载至内存，即读取数据集。
- 将文本拆分成**词元**（单词或者字符）。
- 建立一个词表，用于将**词元映射到数字索引**，在最简单的情况下，词表由文本的所有**唯一词元**组成，数字索引由词元在文本中出现的次数决定，一般情况下，某词元出现的次数越多，其数字索引排名越靠前。
- 将文本转换成数字索引的序列，以方便模型计算。假设文本中由$n$个词元组成，一个词元被映射成一个$m$维**数字索引向量**，那么整个文本就是$n\times m$的二维的**数字索引矩阵**。

## 1.2文本预处理的实现

### 1.2.1读取数据集

定义函数读取“时间机器”这本小说的文本内容。

```
def read_time_machine() -> list[str]:
    with open(d2l.download("time_machine"), mode="r") as f:
        lines = f.readlines()

    return [re.sub("[^A-Za-z]+", " ", line).strip().lower() for line in lines]
```

### 1.2.2文本词元化

将读取到的英文**文本列表**转换为**词元列表**

```
from typing import Literal

def tokenize(
    lines: list[str], token: Literal["word", "char"] = "word"
) -> list[list[str]]:
    if token == "word":
        return [line.split() for line in lines]
    elif token == "char":
        return [list(line) for line in lines]
    else:
        raise f"错误：未知词元类型{token}"
```

### 1.2.3建立词表进行词元映射

利用整个文本建立词表，统计唯一词元出现的次数，得到的统计结果称之为**语料库**。

```
import collections

class Vocab:
    def __init__(self: "Vocab", token=None, min_freq=0, reversed_tokens=None) -> None:
        if token is None:
            token = []
        if reversed_tokens is None:
            reversed_tokens = []

        counter = self.count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
        self.idx_to_token = ["<unk>"] + reversed_tokens
        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}

        for token, freq in self._token_freqs:
            if freq < min_freq:
                break

            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)

        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]

        return [self.idx_to_token[index] for index in indices]

    @property
    def unk(self):
        return None

    @property
    def token_fraqs(self):
        return self._token_freqs

    def count_corpus(self: "Vocab", tokens):
        if len(tokens) == 0 or isinstance(tokens[0], list):
            tokens = [token for line in tokens for token in line]

        return collections.Counter(tokens)
```

### 1.2.4整合所有

```
def load_corpus_time_machine(max_token=-1):
    lines = read_time_machine()
    tokens = tokenize(lines, "char")
    vocab = Vocab(token=tokens)
    corpus = [vocab[token] for line in tokens for token in line]

    if max_token > 0:
        corpus = corpus[:max_token]
  
    return corpus, vocab
```

### 1.2.5数据的随机抽样

在收集好数据之后，如果想要训练模型，同样也需要**小批量**进行训练，因此需要将**长的文本分割成小的子序列**作为**批量**。在模型训练中，**样本**就是从原始序列截取出来的等长**子序列**，一个批量就是把若干个子序列集中在一起。
